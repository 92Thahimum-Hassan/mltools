{"name":"mltools","tagline":"Machine learning tools in MATLAB","body":"Matlab MLTOOLS Toolbox\r\n======================\r\n\r\nThe main objective of this toolbox is to integrate several machine learning methods with in a consistent framework which is coded in a compatible way with the methods available in the other toolboxes on this site. Whilst the other toolboxes typically reflect research projects, this toolbox reflects other researchers work or known algorithms. The code either provides 'wrappers' for other researchers code or independent implementations. Examples of code that is wrapped include Isomap, MVU and NETLAB.\r\n\r\nBackground details for submitted software.\r\n\r\nWe made use of software available from the University of Manchester. To operate our software you must also download the following toolboxes from that site.\r\n\r\n| **Toolbox**                                   | **Version** |\r\n|-----------------------------------------------|-------------|\r\n| [DATASETS](/datasets/downloadFiles/vrs0p1371) | 0.1371      |\r\n| [OPTIMI](/optimi/downloadFiles/vrs0p132)      | 0.132       |\r\n| [NDLUTIL](/ndlutil/downloadFiles/vrs0p163)    | 0.163       |\r\n| [NETLAB](/netlab/downloadFiles/vrs3p3)        | 3.3         |\r\n| [MVU](/mvu/downloadFiles/vrs1p3)              | 1.3         |\r\n| [LMVU](/lmvu/downloadFiles/vrs1p3)            | 1.3         |\r\n| [LLE](/lle/downloadFiles/vrs1p0)              | 1.0         |\r\n| [JDQR](/jdqr/downloadFiles/vrs1p0)            | 1.0         |\r\n| [ISOMAP](/isomap/downloadFiles/vrs1p0)        | 1.0         |\r\n| [SEDUMI](/sedumi/downloadFiles/vrs1p21)       | 1.21        |\r\n\r\nFix to re-enable HGPLVM visualization.\r\n#### Version 0.138\r\n\r\nMinor tweak of model write result and model load result to allow specification of the data loading function.\r\n\r\n#### Version 0.137\r\n\r\nRelease for release of VARGPLVM with dynamics.\r\n\r\n#### Version 0.136\r\n\r\nMinor mods.\r\n\r\n#### Version 0.135\r\n\r\nMinor mods.\r\n\r\n#### Version 0.134\r\n\r\nAdded pmvu model.\r\n\r\n#### Version 0.133\r\n\r\nAdded functionality for writing model files using modelDeconstruct commands to keep written files smaller.\r\n\r\n#### Version 0.132\r\n\r\nAdd click visualise functionality for LVM visualization, Laplacian eigenmaps and wrapper for MVU.\r\n\r\n#### Version 0.1311\r\n\r\nMinor change to lvmScatterPlot to fix bug caused when minimum values were positive.\r\n\r\n#### Version 0.131\r\n\r\nMinor changes to toolbox to fix reading in of C++ code.\r\n\r\n#### Version 0.13\r\n\r\nAdded paramNameRegularExpressionLookup.m to regular expression match a parameter name in a model and return the associated indices. paramNameReverseLookup.m does the same thing but for the specific parameter name. Also added multimodel type, which allows for multi-task style learning of existing models. Added linear mapping type of model.\r\n\r\n#### Version 0.1291\r\n\r\nChanges to modelOutputGrad.m, modelOut.m, kbrOutputGrad.m, kbrExpandParam.m, modelOptimise.m to allow compatibility with SGPLVM and NCCA toolboxes. Added a preliminary coding of LLE.\r\n\r\nNote that to run the LLE code you will need to download the file \"eigs\\_r11.m\"\r\n\r\n#### Version 0.129\r\n\r\nSeveral changes for ICML dimensionality reduction tutorial, including adding density networks and GTM code as well as various latent variable visualisation code such as lvmTwoDPlot and lvmVisualise.\r\n\r\nAdded dnet type model for GTM and density networks. Added various lvm helper files for doing nearest neighbour and plotting results for latent variable models. Added lmvu and mvu embedding wrapper. Added ppca model type. Added output gradients for model out functions (for magnification factor computation in dnet models). Added helpers for reading various models from FID mapmodel, matrix etc.). Added rbfOutputGradX and visualisation for spring dampers type.\r\n\r\n#### Version 0.128\r\n\r\nFixed Viterbi alignment algorithm, thanks to Raquel Urtasun for pointing out the problems with it.\r\n\r\nCarl Henrik Ek added embeddings with maximum variance unfolding (landmark and normal) to the toolbox. Also several files modified by Carl Henrik to allow a single output dimension of a model to be manipulated.\r\n\r\n#### Version 0.127\r\n\r\nMinor modifications including adding file modelAddDynamics to replace fgplvmAddDynamics.\r\n\r\n#### Version 0.126\r\n\r\nModified kbrParamInit to scale alpha weights and biases by number of data. Added 'dynamicsSliderChange' to lvmClassVisualise to allow visulisation of models with 'gpTime' style-dynamics.\r\n\r\n#### Version 0.125\r\n\r\nAdded multimodel for learning multiple indepedent models with shared parameters.\r\n\r\n#### Version 0.124\r\n\r\nAdded periodic RBF network and model gradient checker.\r\n\r\n#### Version 0.123\r\n\r\nMinor release in line with IVM toolbox 0.4.\r\n\r\n#### Version 0.122\r\n\r\nAdded Hessian code for base model type and for MLP. Added Viterbi alignment code, viterbiAlign.\r\n\r\n#### Version 0.121\r\n\r\nVarious minor bug fixes and changes which seem to have gone undocumented.\r\n\r\n#### Version 0.12\r\n\r\nExtended model type to be a generic container module for optimising any model. Added model test for testing a created model. The code is still in a bit of flux though with some design decisions not made and some code untested.\r\n\r\n#### Version 0.111\r\n\r\nFixed bug in kbr where bias parameter fields where still being referred to as b.Also acknowledged the fact that the isomap graph may not be fully connected in isomapEmbed, but don't yet deal with it properly. Finally added lleEmbed.m for wrapping the lle code.\r\n\r\n#### Version 0.11\r\n\r\nUpdated release for operation with FGPLVM toolbox 0.13. Structure of model creation changed and functions of the form modelOptions.m included for setting default options of the various machine learning models.\r\n\r\n#### Version 0.1\r\n\r\nThe first release of the toolbox with various wrappers for NETLAB functions. Also latent variable model visualisation code was moved into this toolbox.\r\n\r\nExamples\r\n--------\r\n\r\n### LLE\r\n\r\n#### The Swiss Roll\r\n\r\nThe 'swiss roll data' is often used to illustrate dimensionality reduction algorithms despite the fact that it is very unrepresentative of real data sets.\r\n\r\nIn the first examples we use 1000 data points to represent the swiss roll, `demSwissRollLle1.m` and `demSwissRollLle2.m`.\r\n\r\n![](demSwissRollLle1.png)![](demSwissRollLle2.png)\r\n *Left*: LLE on the swiss roll data using 4 neighbours. *Right*: LLE on the swiss roll data using 8 neighbours.\r\nIn the next examples we use 3000 data points to represent the swiss roll, `demSwissRollFullLle1.m` and `demSwissRollFullLle2.m`.\r\n\r\n![](demSwissRollFullLle1.png)![](demSwissRollFullLle2.png)\r\n *Left*: LLE on the full swiss roll data using 4 neighbours. *Right*: LLE on the full swiss roll data using 8 neighbours.\r\n#### Oil Data\r\n\r\nThe 'oil data' is commonly used as a bench mark for visualisation algorithms. For more details on the data see [this page](http://www.ncrg.aston.ac.uk/GTM/3PhaseData.html).\r\n\r\nIn these examples we used the 1000 data points from the training data for the oil, `demOilLle1.m` and `demOilLle2.m`.\r\n\r\n![](demOilLle1.png)![](demOilLle2.png)\r\n *Left*: LLE on the oil data using 4 neighbours, 9 errors when using classification by nearest neighbour in the latent space. *Right*: LLE on the oil data using 8 neighbours, 151 errors when using classification by nearest neighbour in the latent space.\r\n\r\nPage updated on Sun Jun 17 08:48:04 2012\r\n","google":"UA-62968536-1","note":"Don't delete this file! It's used internally to help with page regeneration."}